
# Introduction

HTM is essentially a theory on how the human brain functions. Three brain features are critical in the development of  HTM. To begin with, the brain is a hierarchical organization by nature. Signals flow in both ways along the hierarchy. Additionally, there is signal flow within the region. Second, all of the information stored in the brain is temporal. All aspects of brain learning revolve around the concept of time. Finally, the human brain functions primarily as a memory system. Over time, we try to remember and predict patterns. In a way, all of the cells and their connections are storing the patterns that have been observed through time. Humans use the neocortex to learn sequences and predict the future, which is why Hawkins and George (2007) developed hierarchical temporal memory (HTM). It should be able to produce generalized representations for similar inputs in its idealized form. HTM should be able to perform time-dependent regression using its learned representations. Many applications utilizing spatiotemporal data would benefit greatly from such a system. Cui et al. (2016) used HTM to predict taxi passenger counts using time-series data. They used HTM for anomaly detection as well (Lavin and Ahmad 2015). The evolving nature of HTM's algorithmic definition and the lack of a formalized mathematical model have hampered its popularity in the machine learning community. Building a mathematical framework around HTM's original algorithmic definition, this work aims to bridge the gap between neuroscience-inspired algorithms and math-based algorithms.

# Summary of the Project

Hierarchical Temporal Memory (HTM), a machine learning approach that uses Spatial Pooler, Scalar Encoder and Temporal memory and having unit tests on the public functions. Hierarchical Temporal Memory (HTM) theory, which represents the structural and algorithmic aspects of neocortex, has recently developed a new paradigm in machine intelligence. There is still a lot of work to be done on the HTM algorithm's inference of patterns and structures recognized by the algorithm. The agility after testing the sequentially learned data while coinciding them with the input and output is the actual goal.

# Implementation

Through the GetPredictedInputValues() method, I've worked on or tested three public methods. The following is a breakdown of the methods' workings.

Some functions, on the other hand, didn't necessitate unit testing because they don't have any expected return values. The following are examples of some of those techniques. ActiveMap2.Clear() is present in the initial method ClearState, but it lacks the functionality needed to be tested in a unit test. Learn(TIN input, Cell[] output) is the second function's Learn(TIN input, Cell[] output) where the learning process begins after this technique, from whence we acquired the sequences that determine the HTM region parameters.
Scalar encoder, spatial pooler, temporal memory, input, and other encoders all function simultaneously in these locations, and the hierarchical temporal memories play a crucial part in obtaining the required output from the system.

> 1 | public void ClearState()                         |
    |       ^                                          |
  2 |         {                                        |
  3 |             m_ActiveMap2.Clear();                |
  4 |         }                                        |
> 1 | public void Learn(TIN input, Cell[] output)      |
    |                       ^                          |
  2 |         {                                        |
  3 |         }                                        |
    Table 1: Non-returning htmClassifier methods